# STT-LLM-TTS 시스템 흐름도

## 📋 전체 흐름

```
사용자 음성 입력
    ↓
MediaRecorder (WebM 녹음)
    ↓
침묵 감지 → 녹음 중지
    ↓
WebM → MP3 변환
    ↓
/ai/stt (Speech to Text)
    ↓
텍스트 반환
    ↓
/chat-text (LLM 호출)
    ↓
LLM 텍스트 생성
    ↓
텍스트 → 오디오 변환 (TTS)
    ↓
JSON 응답 { text: "...", audio: "base64..." }
    ↓
클라이언트에서 오디오 재생
```

---

## 🎵 WebM을 MP3로 변환하는 이유

### 문제 상황
```javascript
mediaRecorder.ondataavailable = async (event) => {
  const webmBlob = event.data;  // 브라우저는 WebM 형식으로 녹음
  const mp3Blob = await springai.voice.convertWebMToMP3(webmBlob);
}
```

### 변환이 필요한 이유

**침묵으로 인한 녹음 중지 시점**
- 침묵이 감지되면 녹음이 자동 중지됨
- 이 시점에 WebM → MP3 변환이 발생

**STT 모델의 입력 기본값이 MP3**
- OpenAI의 Whisper 등 대부분의 STT 모델은 MP3를 기본 입력 형식으로 사용
- WebM 형식은 호환성이 낮을 수 있음
- MP3는 범용성이 높고 안정적

```javascript
// 변환 과정
침묵 감지 → 녹음 중지 → WebM Blob 생성 → MP3 변환 → STT 전송
```

---

## 🎤 SpeechRecognition의 역할

### 개요
**브라우저 내장 음성 인식 API**로, 음성을 텍스트로 실시간 변환해주는 기능

```javascript
const recognition = new SpeechRecognition();
springai.voice.recognition = recognition;
```

### 주요 설정

#### 1. 언어 설정
```javascript
recognition.lang = 'ko-KR';
```
- 한국어 음성임을 브라우저에게 알려주는 **힌트**
- 명확한 영어도 인식 가능 (절대적인 제약이 아님)

#### 2. 중간 결과 표시
```javascript
recognition.interimResults = true;
```
- `true`: 음성이 확인되면 매번 `onresult` 콜백 호출
- 횟수와 상관없이 계속해서 결과를 전달
- 실시간 피드백 가능

### 자동 종료 동작
```
음성 확인 → 인식 완료 → 음성 인식 기능 자동 종료
```
- 한 번의 음성 입력이 끝나면 자동으로 종료됨
- 지속적인 인식을 위해서는 재시작 필요

### 정확도 문제

#### 왜 사용하는가?
> **"텍스트가 나오는 게 중요하니까"**

- 완벽한 정확도보다 **실시간 피드백**이 우선
- 사용자가 자신의 말을 텍스트로 확인 가능
- 일단 텍스트가 생성되면 충분함
- 최종 변환은 서버의 STT 모델(Whisper)이 담당

```
브라우저 SpeechRecognition (실시간 미리보기)
         +
서버 STT 모델 (정확한 변환)
```

---

## 🔄 requestAnimationFrame 사용 이유

### 코드
```javascript
requestAnimationFrame(springai.voice.checkSilence);
```

### 주 목적

**브라우저 최적화된 주기적 실행**

#### 문제점
- `setInterval`이나 `setTimeout` 사용 시:
  - 브라우저마다 타이밍이 일정하지 않음
  - 백그라운드에서도 계속 실행되어 성능 저하
  - 프레임 드롭 발생 가능

#### 해결책: requestAnimationFrame
- **브라우저의 렌더링 주기에 맞춰 실행** (보통 1초에 60프레임)
- 부드러운 애니메이션 효과
- 탭이 비활성화되면 자동으로 일시 중지 (성능 최적화)
- 재귀 호출로 지속적인 감시 가능

### 실행 빈도
```javascript
// 1초에 약 60번 호출 (브라우저 리프레시 레이트에 따라 다름)
// 일반적으로 25~30 FPS 이상 보장
function checkSilence() {
  // 침묵 감지 로직
  
  // 재귀 호출로 계속 감시
  requestAnimationFrame(checkSilence);
}
```

### 결론
**침묵을 지속적으로 감시하기 위해 `requestAnimationFrame()`을 사용**
- 효율적인 주기적 실행
- 브라우저 최적화
- 부드러운 애니메이션 효과

---

## 🔗 전체 통신 플로우

### 1단계: 음성 → 텍스트 (STT)
```http
POST /ai/stt
Content-Type: multipart/form-data

speech: [MP3 audio file]

→ Response: "안녕하세요"
```

### 2단계: 텍스트 → LLM 응답 + 음성 (Chat)
```http
POST /chat-text
Content-Type: application/json

{
  "userMessage": "안녕하세요"
}

→ Response: {
  "text": "반갑습니다!",
  "audio": "base64_encoded_audio_data..."
}
```

### JSON에서 Base64를 사용하는 이유
- **JSON은 텍스트 기반 형식**이므로 바이너리 데이터를 직접 담을 수 없음
- Base64 인코딩으로 바이너리 오디오를 텍스트로 변환
- 클라이언트에서 Base64 디코딩 후 재생

---

## 📊 시스템 구성 요약

| 단계 | 기술 | 역할 |
|------|------|------|
| 음성 입력 | MediaRecorder | WebM 형식으로 녹음 |
| 침묵 감지 | requestAnimationFrame | 주기적으로 음성 레벨 체크 |
| 형식 변환 | WebM → MP3 | STT 호환성 확보 |
| 실시간 피드백 | SpeechRecognition | 브라우저 내장 음성 인식 |
| 정확한 변환 | STT (Whisper) | 서버 기반 음성 인식 |
| AI 응답 생성 | LLM | 텍스트 응답 생성 |
| 음성 합성 | TTS | 텍스트 → 오디오 변환 |
| 데이터 전송 | Base64 | JSON으로 오디오 전송 |

---

## 💡 핵심 포인트

1. **WebM → MP3**: STT 모델 호환성
2. **SpeechRecognition**: 실시간 피드백용 (정확도 < 속도)
3. **requestAnimationFrame**: 효율적인 침묵 감지
4. **Base64**: JSON에서 오디오 전송
5. **이중 STT**: 브라우저(실시간) + 서버(정확도)